# Model configuration
model_config: configs/models/multigriddet_darknet.yaml

# Dataset paths
data:
  train_annotation: data/coco_train2017.txt
  val_annotation: data/coco_val2017.txt
  classes_path: configs/coco_classes.txt

# Data loader configuration (for GPU optimization)
data_loader:
  use_tf_dataset: true          # Use tf.data.Dataset for better GPU utilization (recommended)
  use_gpu_preprocessing: true    # Use GPU-accelerated preprocessing pipeline (recommended for A100)
  prefetch_buffer: 6             # Prefetch buffer in batches (6 = good GPU-CPU overlap, 'auto' uses AUTOTUNE)
  num_parallel_calls: 24         # Parallel calls for preprocessing (24 = aggressive, 'auto' uses AUTOTUNE)
  shuffle_buffer_size: 8192      # Shuffle buffer size (larger = better randomization, default: 4096)
  interleave_cycle_length: null  # Parallel file reading (null = auto-disable for frozen backbone, or set 8-16 for full training)
  num_workers: 8                 # Number of CPU worker threads for data loading (legacy mode only)
  cache_images: false            # Cache decoded images in memory (optional, uses more RAM)
  
# Training parameters
training:
  batch_size: 4
  epochs: 100
  initial_epoch: 0
  learning_rate: 0.001
  
  # Two-stage training
  transfer_epochs: 0      # Freeze backbone first
  freeze_level: 2          # 0=all, 1=backbone, 2=all but head
  
  # Loss configuration
  loss_option: 2           # 1=IoL-MSE, 2=IoL-MSE+mask, 3=GIoU/DIoU
  label_smoothing: 0.1     # Professional standard: 0.1-0.2 for regularization
  elim_grid_sense: false
  
  # Loss scale parameters
  loss:
    coord_scale: 1.0       # Scale for localization (coordinate) loss
    object_scale: 1.0      # Scale for objectness loss (positive cells)
    no_object_scale: 0.5   # Scale for objectness loss (negative cells)
    class_scale: 1.0       # Scale for classification loss
    anchor_scale: 1.0      # Scale for anchor prediction loss

  # Loss normalization strategy
  # Controls how loss values are normalized to ensure stable training and comparable metrics
  # Options (can combine multiple, they multiply together):
  #   - "batch": Normalize by batch size (default, recommended for consistent scaling)
  #   - "positives": Normalize by number of positive cells (varies per batch, adapts to data)
  #   - "grid": Normalize by batch_size * grid_h * grid_w (total grid cells)
  # Examples:
  #   ["batch"] - Default: normalize by batch size only
  #   ["positives"] - Normalize by number of objects (useful for imbalanced datasets)
  #   ["batch", "grid"] - Normalize by both batch size and grid size
  loss_normalization: ["batch"]  # Default: batch-size normalization


  # Class weights for handling class imbalance
  # Options:
  #   - 'auto': Automatically compute from training data (recommended)
  #   - List of weights: [w0, w1, w2, ...] for each class
  #   - null/omitted: Use equal weights (1.0 for all classes)
  class_weights: auto              # 'auto' to compute from data, or list of weights
  class_weights_method: balanced    # Method for auto-computation: 'balanced', 'inverse', or 'sqrt_inverse'
  
  # Data augmentation
  augmentation:
    enabled: true
    enhance_type: mosaic   # 'mosaic' (YOLOv4 style), 'gridmask', or null
    rescale_interval: -1   # Multi-scale training DISABLED (set to -1 to disable)
  
  # Multi-anchor assignment
  multi_anchor_assign: false
  
# Optimizer
optimizer:
  type: adamw              # Modern standard: AdamW with decoupled weight decay (recommended)
  learning_rate: 0.001
  weight_decay: 0.0005     # Weight decay for AdamW (decoupled from learning rate)
  beta_1: 0.9              # Momentum parameter
  beta_2: 0.999            # Momentum parameter
  
# Learning rate schedule
lr_schedule:
  type: cosine_annealing   # Modern standard: Cosine annealing with warmup (recommended)
  warmup_epochs: 3         # Number of epochs for warmup (gradual LR increase)
  warmup_lr_factor: 0.01  # Start LR = initial_lr * warmup_lr_factor (1% of initial LR)
  min_lr: 1.0e-7           # Minimum learning rate at end of cosine decay
  # Legacy option (if you want to use reduce_on_plateau instead):
  # type: reduce_on_plateau
  # factor: 0.5
  # patience: 3
  # min_lr: 1.0e-7

# Callbacks
callbacks:
  checkpoint:
    save_best_only: true
    monitor: val_loss
    save_dir: logs/checkpoints
  tensorboard:
    log_dir: logs/tensorboard
  early_stopping:
    monitor: val_loss
    patience: 10
    
# Resume training
resume:
  enabled: false
  weights_path: null          # Path to pretrained full model weights (e.g., model5.h5)
  backbone_weights_path: null  # Path to pretrained backbone weights (e.g., darknet53.h5)
  
# Output
output:
  log_dir: logs/training
  model_dir: trained_models
  save_frequency: 1  # Save every N epochs




