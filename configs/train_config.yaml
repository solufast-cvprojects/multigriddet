# Model configuration
model_config: configs/models/multigriddet_darknet.yaml

# Dataset paths
data:
  train_annotation: data/coco_train2017.txt
  val_annotation: data/coco_val2017.txt
  classes_path: configs/coco_classes.txt

# Data loader configuration (for GPU optimization)
data_loader:
  use_tf_dataset: true          # Use tf.data.Dataset for better GPU utilization (recommended)
  use_gpu_preprocessing: true    # Use GPU-accelerated preprocessing pipeline (recommended for A100)
  prefetch_buffer: 6             # Prefetch buffer in batches (6 = good GPU-CPU overlap, 'auto' uses AUTOTUNE)
  num_parallel_calls: 24         # Parallel calls for preprocessing (24 = aggressive, 'auto' uses AUTOTUNE)
  shuffle_buffer_size: 8192      # Shuffle buffer size (larger = better randomization, default: 4096)
  interleave_cycle_length: null  # Parallel file reading (null = auto-disable for frozen backbone, or set 8-16 for full training)
  num_workers: 8                 # Number of CPU worker threads for data loading (legacy mode only)
  cache_images: false            # Cache decoded images in memory (optional, uses more RAM)
  
# Training parameters
training:
  batch_size: 8
  epochs: 100
  initial_epoch: 0
  learning_rate: 0.001
  
  # Two-stage training
  transfer_epochs: 50      # Freeze backbone first
  freeze_level: 1          # 0=all, 1=backbone, 2=all but head
  
  # Loss configuration
  loss_option: 2           # 1=IoL-MSE, 2=IoL-MSE+mask, 3=GIoU/DIoU
  label_smoothing: 0.1     # Professional standard: 0.1-0.2 for regularization
  elim_grid_sense: false
  
  # Data augmentation
  augmentation:
    enabled: true
    enhance_type: mosaic   # 'mosaic' (YOLOv4 style), 'gridmask', or null
    rescale_interval: -1   # Multi-scale training DISABLED (set to -1 to disable)
  
  # Multi-anchor assignment
  multi_anchor_assign: false
  
# Optimizer
optimizer:
  type: adamw              # Modern standard: AdamW with decoupled weight decay (recommended)
  learning_rate: 0.001
  weight_decay: 0.0005     # Weight decay for AdamW (decoupled from learning rate)
  beta_1: 0.9              # Momentum parameter
  beta_2: 0.999            # Momentum parameter
  
# Learning rate schedule
lr_schedule:
  type: cosine_annealing   # Modern standard: Cosine annealing with warmup (recommended)
  warmup_epochs: 3         # Number of epochs for warmup (gradual LR increase)
  warmup_lr_factor: 0.01  # Start LR = initial_lr * warmup_lr_factor (1% of initial LR)
  min_lr: 1.0e-7           # Minimum learning rate at end of cosine decay
  # Legacy option (if you want to use reduce_on_plateau instead):
  # type: reduce_on_plateau
  # factor: 0.5
  # patience: 3
  # min_lr: 1.0e-7

# Callbacks
callbacks:
  checkpoint:
    save_best_only: true
    monitor: val_loss
    save_dir: logs/checkpoints
  tensorboard:
    log_dir: logs/tensorboard
  early_stopping:
    monitor: val_loss
    patience: 10
    
# Resume training
resume:
  enabled: false
  weights_path: null
  
# Output
output:
  log_dir: logs/training
  model_dir: trained_models
  save_frequency: 1  # Save every N epochs





